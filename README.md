# PFE
Study of adversarial, data poisoning, and model extraction attacks on AI systems, focusing on adversarial attacks (FGSM, BIM, PGD, C&amp;W) and defenses such as Adversarial Training, Defensive Distillation, Random Noise Injection, Detection, and Hybrid methods on CNN and MLP models.
