# PFE
ğŸ“„ Overview:

This project analyzes three major classes of attacks on AI systems â€” adversarial attacks, data poisoning, and model extraction â€” with a practical focus on adversarial attacks such as FGSM, BIM, PGD, and C&W.
We evaluate multiple defense strategies including Adversarial Training, Defensive Distillation, Random Noise Injection, Adversarial Example Detection, and a hybrid defense approach.
Experiments are performed on two healthcare datasets:

 Breast Cancer Wisconsin (WDBC) with MLP
 COVID-19 Radiography with CNN

ğŸ§¬ Project Structure:
PFE/
â”‚
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ ğŸ“ MLP/
        â”œâ”€â”€ DetectionOfAdversarialExamplesMLP.ipynb
        â”œâ”€â”€ HybridationMLP.ipynb
        â”œâ”€â”€ Random_noise_injectionMLP.ipynb
        â”œâ”€â”€ adversarialTrainingMLP.ipynb
        â”œâ”€â”€ baselineMLP.ipynb
        â””â”€â”€ defensifDistillationMLP.ipynb      
     
â”‚   â”œâ”€â”€ ğŸ“ CNN/
        â”œâ”€â”€ baselineCNN.ipynb
        â”œâ”€â”€ detection of adversarial training.ipynb
        â”œâ”€â”€ distillation defensif.ipynb
        â”œâ”€â”€ hybridation1_CNN_(resultat1).ipynb
        â”œâ”€â”€ hybridation1_CNN_(resultat2).ipynb
        â”œâ”€â”€ mixing_adversarial_training.ipynb
â”‚       â””â”€â”€ random_noise_injection.ipynb
â”‚
â”œâ”€â”€ ğŸ“ data/              # (Not included due to size limits)
â”‚   â””â”€â”€ README
â”‚
â”œâ”€â”€ ğŸ“ report/
â”‚   â””â”€â”€ PFE_Report.pdf
â”‚
â””â”€â”€ README.md

âš™ï¸ Installation:
All required dependencies are already installed directly inside the notebooks.  
No additional installation steps are needed.

ğŸš€ How to Run:
You can run the notebooks in any Python environment.  
They were originally executed in Google Colab, but they also work locally.

ğŸ—‚ï¸ Datasets:
Breast Cancer Wisconsin (WDBC)
COVID-19 Radiography

ğŸ‘¤ Author:
ELBAHLOULI Oumaima â€“ Master Data Science and Bioinformatics
